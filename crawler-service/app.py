from fastapi import FastAPI
from pydantic import BaseModel
import requests, json, re, time, mimetypes
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from pathlib import Path
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager
import asyncio
from concurrent.futures import ThreadPoolExecutor
from fastapi import FastAPI
from pydantic import BaseModel
import json
from pathlib import Path

# é™„ä»¶è™•ç†éœ€è¦çš„å¥—ä»¶
import pdfplumber
import docx2txt
import io
import docx
import tempfile
import os

app = FastAPI()
BASE_DIR = Path(__file__).resolve().parent
OUTPUT_PATH = BASE_DIR / "output.json"
visited = set()
executor = ThreadPoolExecutor(max_workers=2)  # åŒæ™‚æœ€å¤šè·‘ 2 å€‹åŸå¸‚

class CrawlRequest(BaseModel):
    url: str = None
    city: str
    config: dict = None


# ================== å…±ç”¨å·¥å…·å‡½å¼ ==================
def append_json(path: Path, data: dict):
    """å®‰å…¨å¯«å…¥ json"""
    if path.exists():
        try:
            current = json.loads(path.read_text(encoding="utf-8"))
        except:
            current = []
    else:
        current = []
    current.append(data)
    path.write_text(json.dumps(current, ensure_ascii=False, indent=2), encoding="utf-8")

async def run_in_thread(fn, *args):
    """æŠŠåŒæ­¥å‡½å¼åŒ…æˆç•°æ­¥ï¼Œå¯ç”¨ asyncio.gather ä¸¦è¡Œ"""
    loop = asyncio.get_event_loop()
    return await loop.run_in_executor(executor, fn, *args)

def append_json(path: Path, data: dict):
    if path.exists():
        try:
            current = json.loads(path.read_text(encoding="utf-8"))
        except:
            current = []
    else:
        current = []
    current.append(data)
    path.write_text(json.dumps(current, ensure_ascii=False, indent=2), encoding="utf-8")

# ========== é™„ä»¶è§£æå·¥å…· ==========
def extract_text_from_bytes(content_bytes: bytes, ext: str, paragraphs: int = 3) -> str:
    """ç›´æ¥è§£æ bytesï¼Œä¸å­˜æª”ï¼Œå–å‰ N æ®µæ–‡å­—"""
    text = ""
    try:
        if ext.lower() == ".pdf":
            with pdfplumber.open(io.BytesIO(content_bytes)) as pdf:
                all_text = []
                for page in pdf.pages[:3]:
                    page_text = page.extract_text()
                    if page_text:
                        all_text.append(page_text)
                full_text = "\n\n".join(all_text)
                paragraphs_list = [p.strip() for p in full_text.split("\n\n") if p.strip()]
                text = "\n\n".join(paragraphs_list[:paragraphs])
        elif ext.lower() == ".docx":
            full_text = docx2txt.process(io.BytesIO(content_bytes))
            paragraphs_list = [p.strip() for p in full_text.split("\n\n") if p.strip()]
            text = "\n\n".join(paragraphs_list[:paragraphs])
        elif ext.lower() in [".txt", ".csv"]:
            full_text = content_bytes.decode("utf-8", errors="ignore")
            paragraphs_list = [p.strip() for p in full_text.split("\n\n") if p.strip()]
            text = "\n\n".join(paragraphs_list[:paragraphs])
        else:
            text = "[âš ï¸ ä¸æ”¯æ´çš„é™„ä»¶æ ¼å¼]"
    except Exception as e:
        text = f"[âš ï¸ è§£æå¤±æ•—: {e}]"
    return text.strip()


def download_and_process_attachments(soup: BeautifulSoup, base_url: str, title: str, download_selector: str = None) -> list:
    """ä¸‹è¼‰é™„ä»¶ä½†ä¸å­˜æª”ï¼Œåªå›å‚³æ–‡å­—"""
    attachments = []

    if download_selector:
        if download_selector.strip().endswith("a"):
            elements = soup.select(download_selector)
        else:
            elements = soup.select(f"{download_selector} a")
    else:
        elements = soup.select(".list-text.file-download-multiple ul li a")

    for idx, a in enumerate(elements[:3]):  # åªå–å‰ä¸‰å€‹é™„ä»¶
        try:
            name = a.get_text(strip=True) or f"é™„ä»¶{idx+1}"
            link = urljoin(base_url, a.get("href"))
            r = requests.get(link, timeout=15)
            r.raise_for_status()

            ext = Path(link).suffix or mimetypes.guess_extension(r.headers.get("content-type", "")) or ".bin"
            file_text = extract_text_from_bytes(r.content, ext, 3)
            attachments.append(f"{file_text}")

        except Exception as e:
            attachments.append(f"[ä¸‹è¼‰å¤±æ•—: {e}]")

    return attachments

# ================== å°åŒ—çˆ¬èŸ² (æ”¯æ´é…ç½®åƒæ•¸) ==================
def crawl_taipei(url, city, results: list, config: dict = None):
    global visited
    if url in visited:
        print(f"[DEBUG] URL å·²è¨ªå•: {url}")
        return
    visited.add(url)

    BASE_URL = "https://dosw.gov.taipei/"
    
    # å¾é…ç½®ä¸­å–å¾—é¸æ“‡å™¨ï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨é è¨­
    STOP_SELECTOR = config.get("stopSelector", ".list-text.detail.bottom-detail") if config else ".list-text.detail.bottom-detail"
    LINK_SELECTOR = ".list-text.content-list .h4 a"
    
    if config and "extractSelectors" in config:
        EXTRACT_SELECTORS = config["extractSelectors"]
    else:
        EXTRACT_SELECTORS = {
            "title": "h2.h3",
            "date": ".list-text.detail.bottom-detail ul li:nth-child(2)",
            "content": ".area-editor.user-edit, .area-editor.user-edit td[colspan='3'], .div .essay"
        }

    print(f"[DEBUG] é–‹å§‹çˆ¬å–(å°åŒ—): {url}")
    print(f"[DEBUG] ä½¿ç”¨é…ç½®: stopSelector={STOP_SELECTOR}")
    print(f"[DEBUG] ä½¿ç”¨é…ç½®: extractSelectors={EXTRACT_SELECTORS}")

    try:
        resp = requests.get(url, timeout=10)
        resp.raise_for_status()
        print(f"[DEBUG] ç¶²é å–å¾—æˆåŠŸ: {url}")
    except requests.RequestException as e:
        print(f"[ERROR] çˆ¬å– {url} ç™¼ç”ŸéŒ¯èª¤: {e}")
        return

    resp.encoding = "utf-8"
    soup = BeautifulSoup(resp.text, "html.parser")

    # æª¢æŸ¥æ˜¯å¦æœ‰å­é€£çµéœ€è¦éè¿´
    links = soup.select(LINK_SELECTOR)
    if links:
        print(f"[DEBUG] ç™¼ç¾ {len(links)} å€‹å­é€£çµï¼Œéè¿´çˆ¬å–...")
        for link in links:
            next_url = urljoin(BASE_URL, link.get("href"))
            crawl_taipei(next_url, city, results, config)
    else:
        # æª¢æŸ¥æ˜¯å¦ç‚ºå…§å®¹é é¢
        stop_element = soup.select_one(STOP_SELECTOR)
        if stop_element:
            print(f"[DEBUG] åœæ­¢é¸æ“‡å™¨æ‰¾åˆ°ï¼Œæº–å‚™æŠ“å–å…§å®¹: {url}")
            
            title_el = soup.select_one(EXTRACT_SELECTORS["title"])
            date_el = soup.select_one(EXTRACT_SELECTORS["date"])
            content_elements = soup.select(EXTRACT_SELECTORS["content"])
            
            title = title_el.get_text(strip=True) if title_el else ""
            content = "\n".join([el.get_text(strip=True) for el in content_elements])

            print(f"[DEBUG] æå–åˆ°æ¨™é¡Œ: {title}")
            print(f"[DEBUG] æå–åˆ°å…§å®¹é•·åº¦: {len(content)}")

            # ğŸ†• ä½¿ç”¨é…ç½®çš„é™„ä»¶ä¸‹è¼‰é¸æ“‡å™¨
            download_selector = config.get("downloadData") if config else None
            attachments = download_and_process_attachments(soup, BASE_URL, title, download_selector)
            
            if attachments:
                content += "".join(attachments)
                print(f"[DEBUG] æˆåŠŸæ•´åˆ {len(attachments)} å€‹é™„ä»¶å…§å®¹ï¼ˆå‰ä¸‰æ®µæ–‡å­—ï¼‰")

            # æ—¥æœŸè½‰æ›
            date_text = date_el.get_text(strip=True) if date_el else ""
            print(f"[DEBUG] åŸå§‹æ—¥æœŸæ–‡å­—: {date_text}")
            date_match = re.search(r"\d{3}-\d{2}-\d{2}", date_text)
            date_converted = ""
            if date_match:
                try:
                    parts = date_match.group(0).split("-")
                    roc_year = int(parts[0])
                    year = roc_year + 1911
                    date_converted = f"{year}-{parts[1]}-{parts[2]}"
                    print(f"[DEBUG] è½‰æ›å¾Œæ—¥æœŸ: {date_converted}")
                except Exception as e:
                    print(f"[WARN] æ—¥æœŸè½‰æ›å¤±æ•—: {date_text}, error: {e}")

            data = {
                "city": city,
                "url": url,
                "title": title,
                "date": date_converted,
                "content": content,
            }

            if data["title"] and data["date"] and data["content"]:
                results.append(data)
                append_json(OUTPUT_PATH, data)
                print(f"[INFO] å·²æŠ“å–: {data['title']} ({data['date']}), å…§å®¹é•·åº¦: {len(content)}")
            else:
                print(f"[WARN] è³‡æ–™ä¸å®Œæ•´ï¼Œç•¥é: {url}")
                print(f"[WARN] æ¨™é¡Œ: {bool(data['title'])}, æ—¥æœŸ: {bool(data['date'])}, å…§å®¹: {bool(data['content'])}")
        else:
            print(f"[DEBUG] æœªæ‰¾åˆ°åœæ­¢é¸æ“‡å™¨ '{STOP_SELECTOR}'ï¼Œç•¥é: {url}")

# ================== å—æŠ•çˆ¬èŸ² ==================
def extract_text_from_file(file_url, max_pages=3):
    """ä¸‹è¼‰æª”æ¡ˆä¸¦è§£ææˆæ–‡å­—ï¼ŒåªæŠ“å‰ä¸‰é ã€‚ä¸æ”¯æ´çš„æ ¼å¼è¿”å› None"""
    try:
        # åŠ å¿«ä¸‹è¼‰é€Ÿåº¦ï¼Œæ¸›å°‘ timeout
        res = requests.get(file_url, timeout=10, stream=True)
        if res.status_code != 200:
            return None
        
        # é™åˆ¶ä¸‹è¼‰å¤§å°ï¼ˆé¿å…ä¸‹è¼‰éå¤§æª”æ¡ˆï¼‰
        content = b""
        max_size = 5 * 1024 * 1024  # 5MB
        for chunk in res.iter_content(chunk_size=8192):
            content += chunk
            if len(content) > max_size:
                print(f"[WARN] æª”æ¡ˆéå¤§ï¼Œæˆªæ–·: {file_url[:50]}...")
                break
                
    except Exception as e:
        print(f"[ERROR] ä¸‹è¼‰å¤±æ•—: {str(e)[:50]}")
        return None

    tmp_file = None
    text = ""

    # æ ¹æ“š URL æˆ– Content-Type åˆ¤æ–·æª”æ¡ˆé¡å‹
    content_type = res.headers.get('Content-Type', '').lower()
    file_url_lower = file_url.lower()

    # åœ–ç‰‡æ ¼å¼ç›´æ¥ç•¥é
    if "image" in content_type or file_url_lower.endswith((".jpg", ".jpeg", ".png", ".gif", ".bmp", ".svg")):
        return None

    if file_url_lower.endswith(".docx") or "wordprocessingml" in content_type:
        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".docx")
        tmp_file.write(content)
        tmp_file.close()
        try:
            doc = docx.Document(tmp_file.name)
            paragraphs = [p.text for p in doc.paragraphs if p.text.strip()]
            # åªå–å‰ 30 æ®µï¼ˆå¤§ç´„ 3 é ï¼‰
            text = "\n".join(paragraphs[:30])
        except Exception as e:
            print(f"[ERROR] DOCX è§£æå¤±æ•—: {str(e)[:50]}")
            text = None
            
    elif file_url_lower.endswith(".pdf") or "pdf" in content_type:
        tmp_file = tempfile.NamedTemporaryFile(delete=False, suffix=".pdf")
        tmp_file.write(content)
        tmp_file.close()
        try:
            with pdfplumber.open(tmp_file.name) as pdf:
                pages = pdf.pages[:max_pages]
                text = "\n".join(page.extract_text() or "" for page in pages)
        except Exception as e:
            print(f"[ERROR] PDF è§£æå¤±æ•—: {str(e)[:50]}")
            text = None
    else:
        # ä¸æ”¯æ´çš„æ ¼å¼è¿”å› None
        text = None

    if tmp_file and os.path.exists(tmp_file.name):
        try:
            os.remove(tmp_file.name)
        except:
            pass

    return text.strip() if text else None

def crawl_files_from_table(page_source, base_url, page_url):
    """æ¯å€‹ <tr> æ˜¯ä¸€ç­†è³‡æ–™ï¼Œä¸‹è¼‰ç¬¬ä¸€å€‹é™„ä»¶ä¸¦æ“·å–æ–‡å­—ã€‚ä¸æ”¯æ´æ ¼å¼ç›´æ¥ç•¥é"""
    soup = BeautifulSoup(page_source, "html.parser")
    rows = soup.select("#ListData .table tr")
    results = []

    for tr in rows[1:]:  # è·³éè¡¨é ­
        tds = tr.find_all("td")
        if len(tds) < 3:
            continue

        title = tds[1].get_text(strip=True)
        raw_date = tds[2].get_text(strip=True)
        m = re.match(r"(\d{3})/(\d{2})/(\d{2})", raw_date)
        date = f"{int(m[1])+1911}-{m[2]}-{m[3]}" if m else raw_date

        # æ‰¾ç¬¬ä¸€å€‹é™„ä»¶ï¼ˆä»»ä½•æ ¼å¼ï¼‰
        first_file_tag = None
        for a_tag in tr.select("td a"):
            href = a_tag.get("href", "")
            if href and "FileDownload.ashx" in href:
                first_file_tag = a_tag
                break

        # æ²’æœ‰é™„ä»¶ï¼Œç›´æ¥ç•¥é
        if not first_file_tag:
            print(f"[SKIP] ç„¡é™„ä»¶ï¼Œç•¥é: {title}")
            continue

        file_url = urljoin(base_url, first_file_tag["href"])
        content = extract_text_from_file(file_url, max_pages=3)
        
        # å¦‚æœæ˜¯ä¸æ”¯æ´çš„æ ¼å¼æˆ–åœ–ç‰‡ï¼Œcontent æœƒæ˜¯ Noneï¼Œç›´æ¥ç•¥é
        if content is None:
            print(f"[SKIP] ä¸æ”¯æ´çš„æ ¼å¼ï¼Œç•¥é: {title}")
            continue
        
        # å¦‚æœå…§å®¹æ˜¯ç©ºç™½ï¼Œä¹Ÿç•¥é
        if not content or len(content.strip()) == 0:
            print(f"[SKIP] é™„ä»¶ç„¡å…§å®¹ï¼Œç•¥é: {title}")
            continue

        results.append({
            "city": "å—æŠ•å¸‚",
            "title": title,
            "date": date,
            "content": content,
            "url": page_url  # ä½¿ç”¨ç¶²é  URLï¼Œè€Œéæª”æ¡ˆä¸‹è¼‰é€£çµ
        })

    return results

def crawl_nantou(results: list, visited: set, config: dict = None):
    LIST_URL = "https://welfare.nantou.gov.tw/1486/WebMap/ListView/3654/zh-Hant-TW"
    BASE_URL = "https://welfare.nantou.gov.tw"

    chrome_options = Options()
    chrome_options.add_argument("--headless")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-images")  # ä¸è¼‰å…¥åœ–ç‰‡
    chrome_options.add_argument("--blink-settings=imagesEnabled=false")
    chrome_options.page_load_strategy = 'eager'  # ä¸ç­‰å®Œå…¨è¼‰å…¥

    print("[DEBUG] åˆå§‹åŒ– Chrome WebDriver...")
    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=chrome_options)
    driver.set_page_load_timeout(30)  # è¨­å®šé é¢è¼‰å…¥è¶…æ™‚
    
    try:
        driver.get(LIST_URL)
        time.sleep(2)

        # åªæŠ“å– site-map__list--2 çš„å‰å…­å€‹ li
        soup = BeautifulSoup(driver.page_source, "html.parser")
        main_categories = soup.select(".site-map__list--2 > li")[:6]  # åªå–å‰å…­å€‹
        
        print(f"[DEBUG] æ‰¾åˆ° {len(main_categories)} å€‹ä¸»åˆ†é¡")

        link_urls = []
        for li in main_categories:
            # æŠ“å–è©²ä¸»åˆ†é¡ä¸‹æ‰€æœ‰çš„è©³ç´°é é¢é€£çµï¼ˆsite-map__list--4ï¼‰
            detail_links = li.select(".site-map__list--4 a")
            for a in detail_links:
                href = a.get("href")
                if href and href.startswith("/1486/"):
                    full_url = BASE_URL + href
                    link_urls.append(full_url)
                elif href and href.startswith("http"):
                    link_urls.append(href)

        print(f"[DEBUG] æœ‰æ•ˆé€£çµæ•¸: {len(link_urls)}")

        for idx, detail_url in enumerate(link_urls, 1):
            if detail_url in visited:
                continue
            visited.add(detail_url)

            print(f"[{idx}/{len(link_urls)}] è™•ç†ä¸­: {detail_url}")

            try:
                driver.get(detail_url)
                time.sleep(0.5)  # æ¸›å°‘ç­‰å¾…æ™‚é–“
                soup = BeautifulSoup(driver.page_source, "html.parser")

                title = soup.select_one("h2").get_text(strip=True) if soup.select_one("h2") else ""
                date = ""
                date_tag = soup.select_one("table tr th:contains('ç™¼å¸ƒæ™‚é–“') + td")
                if date_tag:
                    raw_date = date_tag.get_text(strip=True)
                    if re.match(r'^\d{3}/', raw_date):
                        y, mth, d = raw_date.split("/")
                        date = f"{int(y)+1911}-{mth.zfill(2)}-{d.zfill(2)}"
                    else:
                        date = raw_date

                content_tag = soup.select_one("table tr th:contains('å…§å®¹') + td")
                content = content_tag.get_text(strip=True) if content_tag else ""

                # === é—œéµä¿®æ”¹ï¼šå¦‚æœæ˜¯è¡¨æ ¼å‹é é¢ï¼Œç›´æ¥è™•ç†è¡¨æ ¼ä¸­çš„æ¯å€‹ tr ===
                if not title or not content:
                    attachments = crawl_files_from_table(driver.page_source, BASE_URL, detail_url)
                    if attachments:
                        # ä¸è¦åˆä½µï¼Œç›´æ¥æŠŠæ¯å€‹ tr ä½œç‚ºç¨ç«‹çš„ä¸€ç­†è³‡æ–™
                        for att in attachments:
                            data = {
                                "city": att["city"],
                                "title": att["title"],
                                "date": att["date"],
                                "content": att["content"],
                                "url": att["url"]
                            }
                            results.append(data)
                            
                            if config and config.get("OUTPUT_PATH"):
                                append_json(config["OUTPUT_PATH"], data)
                            else:
                                append_json(OUTPUT_PATH, data)
                                
                            print(f"âœ… å·²æŠ“å– (è¡¨æ ¼): {att['title']} ({att['date']}), å…§å®¹é•·åº¦: {len(att['content'])}")
                        
                        # è¡¨æ ¼è™•ç†å®Œç•¢ï¼Œè·³åˆ°ä¸‹ä¸€å€‹ detail_url
                        continue

                # === å¦‚æœä¸æ˜¯è¡¨æ ¼å‹é é¢ï¼Œä»æŒ‰åŸæœ¬é‚è¼¯è™•ç† ===
                if not title or not content:
                    print(f"[WARN] ç„¡æ³•æŠ“åˆ°ä»»ä½•å…§å®¹ï¼Œç•¥é: {detail_url}")
                    continue

                data = {
                    "city": "å—æŠ•å¸‚",
                    "title": title,
                    "date": date,
                    "content": content,
                    "url": detail_url
                }

                results.append(data)
                if config and config.get("OUTPUT_PATH"):
                    append_json(config["OUTPUT_PATH"], data)
                else:
                    append_json(OUTPUT_PATH, data)
                    
                print(f"âœ… å·²æŠ“å–: {title} ({date}), å…§å®¹é•·åº¦: {len(content)}")

            except Exception as e:
                print(f"âš ï¸ æŠ“å–å¤±æ•—: {detail_url} - {e}")
                continue

    finally:
        driver.quit()
        
    print(f"[INFO] å—æŠ•çˆ¬å–å®Œæˆï¼Œå…±æŠ“åˆ° {len(results)} ç­†è³‡æ–™")

# ================== FastAPI å…¥å£ ==================
@app.post("/crawl")
async def crawl_endpoint(req: CrawlRequest):
    global visited
    visited = set()
    
    OUTPUT_PATH.parent.mkdir(exist_ok=True)
    OUTPUT_PATH.write_text("[]", encoding="utf-8")

    results = []

    tasks = []

    # å°åŒ—çˆ¬èŸ²
    if req.city in ["å°åŒ—å¸‚", "è‡ºåŒ—å¸‚", "taipei"]:
        if not req.url:
            return {"success": False, "message": "å°åŒ—å¸‚å¿…é ˆæä¾› url", "data": []}
        tasks.append(run_in_thread(
            crawl_taipei, req.url, req.config.get("city", "è‡ºåŒ—å¸‚") if req.config else "è‡ºåŒ—å¸‚", results, req.config
        ))

    # å—æŠ•çˆ¬èŸ²
    if req.city in ["å—æŠ•å¸‚", "å—æŠ•ç¸£", "nantou"]:
        tasks.append(run_in_thread(crawl_nantou, results, visited, req.config))

    if not tasks:
        return {"success": False, "message": f"ç›®å‰ä¸æ”¯æ´ {req.city}", "data": []}

    # åŒæ™‚è·‘
    await asyncio.gather(*tasks)

    # é©—è­‰è¼¸å‡º
    try:
        if OUTPUT_PATH.exists():
            saved_data = json.loads(OUTPUT_PATH.read_text(encoding="utf-8"))
            print(f"[DEBUG] output.json å…±æœ‰ {len(saved_data)} ç­†è³‡æ–™")
            for i, item in enumerate(saved_data[:3]):
                print(f"[DEBUG] ç¬¬{i+1}ç­†: {item.get('title', 'N/A')[:50]}...")
    except Exception as e:
        print(f"[WARN] ç„¡æ³•è®€å– output.json: {e}")

    return {"success": True, "city": req.city, "count": len(results), "data": results, "output_file": str(OUTPUT_PATH.absolute())}